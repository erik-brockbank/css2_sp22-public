{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c7e6f5",
   "metadata": {},
   "source": [
    "# Lecture 17 (5/4/2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea5e3d",
   "metadata": {},
   "source": [
    "**Announcements**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac90fc6",
   "metadata": {},
   "source": [
    "*Last time we covered:*\n",
    "- Prediction: regression + overfitting\n",
    "\n",
    "**Today's agenda:**\n",
    "- Classification! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ebecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67da52",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "- What problem it solves\n",
    "- Contrast with regression\n",
    "- Context: widespread, huge part of most machine learning curricula\n",
    "- Goals: data-based (doesn't require specification by a human), accurate incl. w/ new data, fair (!!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6baff4a",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "Ed lec 7 \"problem statement\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92fb44",
   "metadata": {},
   "source": [
    "## Examples\n",
    "- *Binary* classification\n",
    "- *Multiclass* classification\n",
    "- Other: *one-class*, *multi-label*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- For each one: how do people solve this? how might you build an algorithm to solve it?\n",
    "\n",
    "\n",
    "Binary prediction\n",
    "- Toy: Titanic, spam emails, CAPTCHA; real world: risk assessment tools, loan defaults, voter behavior, ...\n",
    "\n",
    "Generalizing to multiclass\n",
    "- Toy: MNIST; real world: face recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429b3b8",
   "metadata": {},
   "source": [
    "### Binary classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e6ecbf",
   "metadata": {},
   "source": [
    "![recidivism_classifier](img/recidivism.png)\n",
    "([source](https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/), see also [here](https://www.nytimes.com/2020/02/06/technology/predictive-algorithms-crime.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d38ae",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979bc58",
   "metadata": {},
   "source": [
    "![fb](img/fb.png)\n",
    "\n",
    "([source](https://www.nytimes.com/2021/11/02/technology/facebook-facial-recognition.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d1ebc7",
   "metadata": {},
   "source": [
    "### Other types of classification (less common)\n",
    "\n",
    "We won't discuss these in detail but it's helpful to know they exist! \n",
    "\n",
    "**Multi-label classification**: Decide which among a set of multiple labels should be applied to a particular instance \n",
    "- *Does this image contain a cat? A person? A car? All the above?*\n",
    "\n",
    "**One-class classification**: Try to optimally identify an instance of a single class \n",
    "- *Is this datapoint an outlier? Does this medical scan contain a tumor?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0214a914",
   "metadata": {},
   "source": [
    "## Intuitive solution: $k$-nearest neighbors (k-NN)\n",
    "\n",
    "- How it works\n",
    "- `sklearn` implementation\n",
    "- Try it out!\n",
    "- How well does it perform? *...next time!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb7c17",
   "metadata": {},
   "source": [
    "### How it works\n",
    "\n",
    "![mean_girls_cafeteria](img/mean_girls.jpeg)\n",
    "\n",
    "*Birds of a feather flock together*: the k-NN classifier guesses an item's label based on whatever training data the item is *closest to*.\n",
    "\n",
    "\n",
    "**Pseudo-code**\n",
    "\n",
    "Steps for classifying a *new* example $x$ with *unknown* label $y$ (more [here](https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761)):\n",
    "1. Cycle through all *known* data points $x_i$ with *known label* $y_i$.\n",
    "2. For each known data point $x_i$, calculate and store the *distance* from $x_i$ to our unknown data point $x$ (Note: this algorithm relies on a distance function $D(x_1, x_2)$)\n",
    "3. After cycling through all known data points, select the $k$ data points with the *lowest* distance to our unknown data point $x$. These are the \"nearest neighbors.\" We choose $k$ ahead of time!\n",
    "4. Select an estimated label $\\hat{y}$ for $x$ that corresponds to the *most common label* $y$ among the nearest data points from step 3.\n",
    "\n",
    "\n",
    "![knn_wikipedia](img/knn.png)\n",
    "\n",
    "In the image above, we're trying to guess a classification (*red triangle* or *blue square* for the green circle. The black outlines show our *nearest neighbors* for $k = 3$ and $k = 5$ ([source](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm))\n",
    "\n",
    "Let's see what this looks like in practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcfc6af",
   "metadata": {},
   "source": [
    "### Implementing k-NN classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The iris dataset is a popular dataset for illustrating classification algorithms\n",
    "iris = sns.load_dataset('iris')\n",
    "iris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at whether petal length and petal width allow for straightforward k-NN classification\n",
    "\n",
    "sns.scatterplot(data = iris, x = \"petal_length\", y = \"petal_width\", hue = \"species\", alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096897c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set aside some test data that we'll classify\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_vals = iris.loc[:, ('petal_length', 'petal_width')]\n",
    "y_vals = iris.loc[:, 'species']\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x_vals, \n",
    "                                               y_vals, \n",
    "                                               test_size = 0.1, # we'll use a small test set here\n",
    "                                               random_state = 0\n",
    "                                               )\n",
    "\n",
    "xtrain = xtrain.reset_index(drop = True)\n",
    "xtest = xtest.reset_index(drop = True)\n",
    "ytrain = ytrain.reset_index(drop = True)\n",
    "ytest = ytest.reset_index(drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ea2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For illustration we'll pick a single element from the test set to classify\n",
    "test_x = xtest.iloc[8, :]\n",
    "\n",
    "# Where is it alongside the training data?\n",
    "sns.scatterplot(data = xtrain, x = \"petal_length\", y = \"petal_width\", hue = ytrain, alpha = 0.5)\n",
    "plt.text(s = \"X\", x = test_x[0], y = test_x[1], horizontalalignment = 'left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e94dfe",
   "metadata": {},
   "source": [
    "*Can we classify the element labeled with the X above?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For distance, we'll use Euclidean distance\n",
    "xtrain['distance'] = np.sqrt((xtrain['petal_length'] - test_x[0])**2 + (xtrain['petal_width'] - test_x[1])**2)\n",
    "xtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2fb69f",
   "metadata": {},
   "source": [
    "![knn_distance](img/knn_dist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be86de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our k value\n",
    "k = 3\n",
    "\n",
    "# What are the values for the k smallest distances?\n",
    "xtrain.nsmallest(n = k, columns = 'distance', keep = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d3732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the classifications of these nearest neighbor items?\n",
    "ytrain.iloc[list(xtrain.nsmallest(n = k, columns = 'distance').index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea105a75",
   "metadata": {},
   "source": [
    "Our $k$-nearest neighbors above classified our test item as `versicolor`.\n",
    "\n",
    "*Was this classification accurate?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b31cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = ytest.iloc[8]\n",
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba25e5",
   "metadata": {},
   "source": [
    "*What if we change $k$ to 5? 10? 25?*\n",
    "\n",
    "The process above should give you an intuition for what the k-NN classifier needs to do to classify novel test data. Fortunately, the `sklearn` `KNeighborsClassifier` will do most of this work for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c901ebdf",
   "metadata": {},
   "source": [
    "### k-NN classification with `sklearn`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a10f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the classifier (use the same `k` as above)\n",
    "knn = KNeighborsClassifier(n_neighbors = k)\n",
    "\n",
    "# Re-generate the train/test data we used above\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x_vals, \n",
    "                                                y_vals, \n",
    "                                                test_size = 0.1, # we'll use a small test set here\n",
    "                                                random_state = 0\n",
    "                                               ) \n",
    "\n",
    "# Use the model's `fit` function to train the classifier\n",
    "# Question: what is this \"fit\" function doing for kNN?\n",
    "knn.fit(X = xtrain, y = ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's predict our test elements! \n",
    "# (we can do them all at once instead of one at a time as we did above...)\n",
    "\n",
    "# reminder: this is what our test data looks like\n",
    "xtest\n",
    "ytest\n",
    "\n",
    "# use the model's `predict` function to make predictions about xtest\n",
    "preds = knn.predict(X = xtest)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0465e7",
   "metadata": {},
   "source": [
    "*How did we do?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_eval = pd.DataFrame({\n",
    "    \"petal_length\": xtest['petal_length'],\n",
    "    \"petal_width\": xtest['petal_width'],\n",
    "    \"species\": ytest,\n",
    "    \"knn prediction\": preds\n",
    "})\n",
    "\n",
    "knn_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacf5761",
   "metadata": {},
   "source": [
    "### Your turn! \n",
    "\n",
    "Use the `sklearn` `KNeighborsClassifier` to classify a set of held out test data.\n",
    "\n",
    "But this time, use `sepal_length` and `sepal_width` as the **features** instead of *petal length* and *petal width* as we did above. This makes things a little more interesting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = iris, x = \"sepal_length\", y = \"sepal_width\", hue = \"species\", alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0727b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9460fc04",
   "metadata": {},
   "source": [
    "## Closing thoughts\n",
    "\n",
    "In the above, we showed how the $k$-nearest neighbors classifier generates predictions for classifying novel data.\n",
    "\n",
    "Next week, we'll look at other solutions to this problem and how to compare them.\n",
    "\n",
    "But first, we need to cover an important question for *any* classifier: how do we evaluate the output to determine whether it did a good job (or, e.g., choose the best $k$ value)?\n",
    "\n",
    "This is what we'll discuss on Friday!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3d210",
   "metadata": {},
   "source": [
    "## Classification by hand\n",
    "\n",
    "Pick a threshold and evaluate TP/FP etc. \n",
    "Or pick a loss function (FP very bad!) and find optimal threshold?\n",
    "Vul lecture 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f037ae8",
   "metadata": {},
   "source": [
    "## Classification models\n",
    "- K nearest neighbors\n",
    "- Logistic regression\n",
    "- Decision trees, SVMs, discriminant analysis, neural networks, random forests, gradient boosted trees \n",
    "\n",
    "vul lecture 8, Garrett lecture 16, 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83157fc6",
   "metadata": {},
   "source": [
    "## Evaluating classification\n",
    "- confusion matrix (Garrett binary classification ppt)\n",
    "- FP, TP, FA, FN, ... (Garrett binary classification ppt)\n",
    "- Metrics: accuracy, precision, F1, ... (Garrett binary classification ppt)\n",
    "- Thresholds and ROC curves: evaluating ROC curves (AUC) (Garrett lec 15)\n",
    "\n",
    "Vul lecture 7, 7-extras; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec065d1",
   "metadata": {},
   "source": [
    "## Classification: understanding the problem\n",
    "- Common examples: labels (MNIST, 538 margaritas? social science example?), binary (titanic, but also loan defaults, voter behavior!)\n",
    "- Solutions: use rules (decision tree), learn a function (logistic, neural networks), draw a line (svm, discriminant), use clustering info (knn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
