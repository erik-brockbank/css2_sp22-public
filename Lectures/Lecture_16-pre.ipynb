{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c7e6f5",
   "metadata": {},
   "source": [
    "# Lecture 16 (5/2/2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea5e3d",
   "metadata": {},
   "source": [
    "**Announcements**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac90fc6",
   "metadata": {},
   "source": [
    "*Last time we covered:*\n",
    "- Linear regression\n",
    "    - Simple linear regression\n",
    "    - Evaluating regression\n",
    "    - Polynomial and multiple regression\n",
    "\n",
    "**Today's agenda:**\n",
    "- Multiple regression continued\n",
    "- Overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ebecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3d210",
   "metadata": {},
   "source": [
    "# Multiple Regression\n",
    "\n",
    "\n",
    "## Big idea: multiple predictors for $y$\n",
    "\n",
    "In real-world scenarios, it's common to explore the relationship between *multiple* predictor variables that all jointly impact the dependent variable. \n",
    "\n",
    "For this, we need **multiple regression**. \n",
    "\n",
    "Though the guiding principles are similar to *simple regression*, things can get much more complicated in multiple dimensions. Here, we'll provide enough of an overview for you to be able to use multiple regression as part of your modeling toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad3133",
   "metadata": {},
   "source": [
    "## Example: predicting life expectancy\n",
    "\n",
    "Let's start with a familiar example.\n",
    "\n",
    "In Problem Set 3 and elsewhere, we've looked at the *gapminder* dataset, which explores the relationship between GDP, population, and life expectancy at the national level and over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dbfdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap = pd.read_csv(\"https://raw.githubusercontent.com/UCSD-CSS-002/ucsd-css-002.github.io/master/datasets/gapminder.csv\")\n",
    "\n",
    "# Let's keep just some of the variables (note for pset!)\n",
    "gap_subset = gap.loc[gap['year'] == 2007, ('country', 'year', 'lifeExp', 'pop', 'gdpPercap')]\n",
    "\n",
    "# Add log population\n",
    "gap_subset['logPop'] = np.log10(gap_subset['pop'])\n",
    "gap_subset['logGdpPercap'] = np.log10(gap_subset['gdpPercap'])\n",
    "gap_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9062b1",
   "metadata": {},
   "source": [
    "In Problem Set 3, you generated a graph that explored the relationship between income (GDP per-capita on $x$) and life expectancy (in years on $y$), alongside two additional predictors: region (color) and population (size).\n",
    "\n",
    "![gap](img/gapminder.png)\n",
    "\n",
    "Here, we want to know formally what the relationship is between the three continuous variables (income, life expectancy, and population). In other words, **can we predict life expectancy using both income *and* population better than we could only using one of those variables?**\n",
    "\n",
    "Let's start by just examining each of the predictors in isolation to see if this is a plausible hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dafd813",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lmplot(data = gap_subset, \n",
    "                    x = \"logGdpPercap\", # x1 \n",
    "                    y = \"lifeExp\"\n",
    "                   )\n",
    "plt.title(\"Life expectancy as a function of income\")\n",
    "plt.xlabel(\"Income (log GDP / capita)\")\n",
    "plt.ylabel(\"Life expectancy\")\n",
    "plt.show()\n",
    "\n",
    "h = sns.lmplot(data = gap_subset, \n",
    "                    x = \"logPop\", # x2 \n",
    "                    y = \"lifeExp\"\n",
    "                   )\n",
    "plt.title(\"Life expectancy as a function of population\")\n",
    "plt.xlabel(\"Population (log)\")\n",
    "plt.ylabel(\"Life expectancy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37200f8",
   "metadata": {},
   "source": [
    "One of these variables has a strong positive relationship. The other one seems a bit less clear. \n",
    "\n",
    "How can we think about exploring the role that both of them play together in predicting life expectancy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771bdac7",
   "metadata": {},
   "source": [
    "### Multiple regression: overview\n",
    "\n",
    "Multiple regression is like linear regression except that we assume our dependent variable $y_i$ is *jointly* predicted by multiple independent variables: \n",
    "\n",
    "$x_1$, $x_2$, ..., $x_n$.\n",
    "\n",
    "Simple linear regression assumes that our data $(x_i, y_i)$ has the following form:\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$\n",
    "\n",
    "With multiple regression, we now extend this model to include multiple predictors for our data ($x_{1i}$, $x_{2i}$, ..., $x_{ni}$, $y_i$):\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ ... \\ + \\beta_n x_{ni} + \\epsilon_i $\n",
    "\n",
    "In most cases, multiple regression once again uses the same *Ordinary Least Squares* (OLS) parameter estimation as simple regression. However, interpreting the parameter estimates is a little less straightforward.\n",
    "\n",
    "*How would we interpret $\\beta_0 = 1$, $\\beta_1 = 2$, $\\beta_2 = 3$?*\n",
    "\n",
    "(Think of a simple example. What variables would you want to use to predict somebody's height *as accurately as possible*? Now imagine each of those variables is one of your $x_{ji}$ variables.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a8290",
   "metadata": {},
   "source": [
    "## Multiple regression in python\n",
    "\n",
    "The scikit-learn `LinearRegression` class is very easy to extend to multiple regression!\n",
    "\n",
    "We'll also demonstrate the `statsmodels` approach below for more robust statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8dd1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit learn approach\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x_vals = np.array(gap_subset[['logGdpPercap', 'logPop']]) # Note: this double bracket syntax is important!\n",
    "x_vals = x_vals.reshape(len(gap_subset), 2)\n",
    "x_vals\n",
    "\n",
    "y_vals = np.array(gap_subset['lifeExp'])\n",
    "y_vals\n",
    "\n",
    "mod = LinearRegression().fit(X = x_vals, y = y_vals)\n",
    "\n",
    "mod.intercept_\n",
    "mod.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b63367",
   "metadata": {},
   "source": [
    "What do the coefficient estimates above suggest? \n",
    "\n",
    "How should we interpret them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1847b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 for our regression\n",
    "mod.score(X = x_vals, y = y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we don't include the population predictor?\n",
    "x_single = np.array(gap_subset['logGdpPercap']) # Note: this double bracket syntax is important!\n",
    "x_single = x_single.reshape(len(gap_subset), 1)\n",
    "x_single\n",
    "\n",
    "mod_simple = LinearRegression().fit(X = x_single, y = y_vals)\n",
    "mod_simple.score(X = x_single, y = y_vals)\n",
    "\n",
    "# Doesn't seem like population helps much..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9d518",
   "metadata": {},
   "source": [
    "The statsmodels approach gives us a bit more insight here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec729dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statsmodels approach\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "multiple_reg = smf.ols('lifeExp ~ logGdpPercap + logPop', data = gap_subset).fit()\n",
    "\n",
    "# View the results\n",
    "multiple_reg.summary()\n",
    "\n",
    "# Our population variable does have a significant positive slope, \n",
    "# but it's pretty small and the effect may be driven by outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b2bb1",
   "metadata": {},
   "source": [
    "# Regression: wrap-up\n",
    "\n",
    "Regression is one of the most powerful tools in our data science / analysis toolkit. \n",
    "\n",
    "Many problems with understanding and predicting data can be distilled down to basic regression.\n",
    "\n",
    "However, it's important to be familiar with the *limitations* in regression as well. \n",
    "\n",
    "One class of these limitations is when **the data violate the assumptions of linear regression**. We're *not* going to get into these issues in this class, but it's good to be aware that not all data (even data that looks linear!) can be accurately described by linear regression. There are a number of tricks for diagnosing this, such as plotting your residuals (we may explore this on the problem set as a way to cover some of this material).\n",
    "\n",
    "However, an important limitation that we *will* discuss now is when **regression overfits the data**. Avoiding overfitting is something that arises in virtually all modeling contexts. It's easiest to illustrate with regression, but the things we discuss here will be relevant throughout the remainder of the quarter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f721881b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6392c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
